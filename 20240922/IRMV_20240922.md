# 第一周汇报 20240922

## 1 论文阅读

阅读了以下三篇文献：

| 序号 | 标题                                                         | 说明                                                         |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D | LSS：多视角图像基于深度分布的锥状体投影转BEV空间的方法可以借鉴 |
| 2    | BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework **(BEVFusion1)** | 第一篇BEVFusion<br />最后的FusionModule强化BEV显著特征的方法可以借鉴 |
| 3    | BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation **(BEVFusion2)** | 第二篇BEVFusion<br />其中对LSS池化步骤的改进以减少计算成本的方法可以借鉴 |

之所以要将Camera Stream和LiDAR Stream放到BEV空间中融合处理，主要有以下三点原因：

- 放在LiDAR空间处理会造成几何信息损失；
- 放在Camera空间处理会造成语义信息损失；
- 而BEV可以在不损失重要信息的情况下处理自动驾驶中的传感融合任务。

![bevfusion2_04](.\assets\bevfusion2_04.png)

另外，综合来看，有一些感悟：

- 把上述两篇BEVFusion的优势结合起来，将BEVFusion1中的FusionModule应用到BEVFusion2中可能会有不一样的效果。
- 另外他们的LiDAR Stream中基本上都是应用的基于voxel的方法，这的确有为了应用CNN等对输入格式有严格要求的网络的原因，但是PointNet++对点云做了sample和group后形成的输入也有特定的格式，那么可否将PointNet++中sample+group的中间结果替代voxel方法输入给网络训练呢？

###  1.1 BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework

这是本文各个分支的使用的一些模块（可能不太全）。

| Camera Stream         | LiDAR Stream |
| --------------------- | ------------ |
| LSS -> Dual-Swin-Tiny | PointPillars |
| FPN                   | CenterPoint  |
| -                     | TransFusion  |

主要流程如下：

![bevfusion1_02](.\assets\bevfusion1_02.png)

Fusion Module如下，采用平均池化和sigmoid激活的方法，得到每个元素值在[0,1]权重矩阵，与原信息点积后做到对显著信息的筛选和增强。

![bevfusion1_03](.\assets\bevfusion1_03.png)

### 1.2 BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation

这是本文各个分支的使用的一些模块（可能不太全）。

| Camera Stream | LiDAR Stream |
| ------------- | ------------ |
| Swin-T        | VoxelNet     |
| LSS(improved) |              |

主要流程如下：

![bevfusion2_02](.\assets\bevfusion2_02.png)

基于LSS改进的BEV Pooling步骤如下：

主要使用GPU kernel和GPU thread对每个grid进行处理，既减少了DRAM的使用，又加快了速度。

![bevfusion2_03](.\assets\bevfusion2_03.png)

这一步的具体处理流程目前理解的还不是很透彻，计划下周通过阅读代码的方式加深理解。

### 1.3 Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D

因为以上两篇文章中对Camera Stream的处理上都有LSS的影子，因而尝试通过对LSS原文的阅读加深对Camera Stream处理的理解。



①Lift: Latent Depth Distribution流程：

主要是得出像素级的深度分布α以及语义信息c，两者做外积，作为multi-plane image的通道方向的值。

![lss_02](.\assets\lss_02.png)

②Splat: Pillar Pooling流程：

对每个相机得到的经lift处理后的锥状体信息经相机内外参融合后转化为BEV语义信息。

![lss_03](.\assets\lss_03.png)

# 2 nuScences数据集学习

因为论文中基本上都以nuScences作为主要的数据源，而且后续产出自己的demo也计划基于这个数据集，因而就对nuScences数据集的组织结构、nuscence-devkit的调用方式，尤其是对其中各个模块之间token的链接关系做了研究：

![nuScene02](.\assets\nuScene02.png)

这周学习研究的方式主要是用jupyternotebook的方式对每一个模块(scene, sample, sample_data, annotation...)进行了学习（已折叠）：

![nuScene01](.\assets\nuScene01.png)

## 3 BEVFusion代码入门

所使用的开源代码基于BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation这篇论文。

第一周所做工作主要是配置环境(torch+mmcv+mmdet+mmdet3等)，跑通代码。

项目代码的目录如下，其中框起来的部分为数据集、训练得到的权重文件、测试得到的可视化文件、测试得到的指标文件。

![content](.\assets\content.png)

跑通后真实值gt的可视化如下：

| camera0                                | lidar                              | map                            |
| -------------------------------------- | ---------------------------------- | ------------------------------ |
| ![camera0_gt](.\assets\camera0_gt.png) | ![lidar_gt](.\assets\lidar_gt.png) | ![map_gt](.\assets\map_gt.png) |

用nuScences v1.0-mini的数据训练了两轮结果如下：（尝试一下，跑通就行，因为轮次少，效果非常不好）：

| camera0                                        | lidar                                      |
| ---------------------------------------------- | ------------------------------------------ |
| ![camera0_custom](.\assets\camera0_custom.png) | ![lidar_custom](.\assets\lidar_custom.png) |

## 4 下周计划

呃，因而这周主要是在阅读论文和配置环境，实际可用的代码结果并不多，这里就不附上代码和模型文件了，待下周对源码有了进一步的理解，做出些实际的demo后再附件。

### 4.1 BEVFusion开源项目进一步研究

- 继续研究基于BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation的代码;
- 重点1：关于基于LSS的Camera Stream处理转化为BEV特征信息的部分;

- 重点2：关于数据导入的部分;
- 重点3：关于训练的部分;
- 重点4：关于如何配置Camera Stream和LiDAR Stream所使用的模块的部分。

### 4.2 Transformer学习

因为许多Camera Stream和LiDAR Stream中的模块，包括后续可能要继续学习的BEVFormer模块中都要用到Transformer，所以计划从基础的Transformer（动手学深度学习 李沐）开始学习，主要是了解其主要思想，基本结构，基本术语和应用方式。